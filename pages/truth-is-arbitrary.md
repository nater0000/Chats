---
title: "Truth is Arbitrary"
author: "Nathan R"
date: 2025-05-27
time: 07:12 PM
terminal: "iPhone"
gpt: "Gemini"
tags: [gpt, truth, gemini, information]
layout: gpt-log
---

# Truth is Arbitrary

<p class="terminal-line matrix user">user@local:~$</p>

When I say “truth is arbitrary” from an information theory perspective, what do I mean?

<p class="terminal-line matrix gpt">gpt@remote:~$</p>

When you say "truth is arbitrary" from an information theory perspective, you're likely pointing to the idea that the meaning or truth-value we assign to symbols or data is not inherent in the symbols themselves, but is a matter of convention, context, and the shared understanding (or "codebook") between the sender and receiver.
Here's a breakdown of what this means from an information theory standpoint:
 * Information as Reduction of Uncertainty (Shannon Information):
   * Classical information theory, pioneered by Claude Shannon, primarily deals with the quantity of information, not its meaning or truth.
   * Information is defined as the reduction of uncertainty. A message conveys information if it tells you something you didn't already know, reducing the number of possible states the world could be in.
   * For example, the sequence of bits "01000001" has a certain amount of Shannon information regardless of whether it represents the letter 'A' (in ASCII), the number 65, or something entirely different in another coding scheme.
   * Arbitrariness: The "truth" that "01000001" is 'A' is arbitrary in the sense that we could have chosen a different bit pattern to represent 'A'. The mapping is a human convention.
 * Symbols and Referents:
   * Information is conveyed through symbols (bits, letters, sounds, etc.). These symbols gain meaning by referring to something else (objects, concepts, ideas).
   * The link between a symbol and its referent (what it "means" or what "truth" it points to) is established by a shared code or context.
   * Arbitrariness: The word "dog" refers to a furry, four-legged canine because English speakers agree on this convention. In French, it's "chien"; in Spanish, "perro." None of these sequences of sounds or letters are inherently "dog-like." The truth of "this is a dog" when pointing to the animal is tied to this agreed-upon linguistic mapping.
 * The Importance of the Codebook/Context:
   * For information to be meaningfully interpreted (and for its "truth" to be assessed), both the sender and receiver must share the same codebook or understand the same context.
   * If I send you an encrypted message, the string of characters is meaningless (its "truth" is inaccessible) until you apply the correct decryption key (the codebook). Once decrypted, the message might state a fact whose truth can then be evaluated based on external reality and the semantic conventions of the language used.
   * Arbitrariness: The "truth" of the decrypted message is still dependent on the arbitrary choice of encryption key and the arbitrary conventions of the language it's written in. A different key would yield gibberish or a different (false) message.
 * Semantic Information vs. Syntactic Information:
   * Shannon's theory is largely syntactic (about the structure and transmission of symbols). Semantic information deals with meaning and truth.
   * While information theory doesn't directly define "truth," it highlights that the "truth-value" of a piece of information is an interpretation layered on top of the raw data. This interpretation depends on a semantic framework that itself is a constructed system.
   * Arbitrariness: Different semantic frameworks or models can be applied to the same raw data, potentially leading to different interpretations of its "truth" or significance. For instance, the same set of economic data points might be interpreted by one economist as signaling growth (a "true" sign of recovery) and by another as a precursor to a bubble (a "true" sign of impending problems), based on their differing economic models (their interpretative frameworks).
 * No Absolute "Ground Truth" Within the Signal Itself:
   * The signal (the bits, the waveform) doesn't carry its own "truth certificate." Its interpretation as true or false relies on external factors:
     * The agreed-upon meaning of the symbols.
     * The reliability of the source.
     * Correspondence with some external reality or agreed-upon model of reality.
   * Arbitrariness: The conventions used to establish these external factors are, at some level, arbitrary human constructs. We've decided what symbols mean and what constitutes a reliable source or a valid model.
Therefore, saying "truth is arbitrary" from an information theory perspective means that the assignment of meaning and truth-value to a piece of information (a string of symbols) is not an intrinsic property of that information but is contingent upon the representational systems, codes, and contexts established and shared by interpreters. The raw information itself is neutral; its "truth" emerges from the framework we use to decode and understand it.



